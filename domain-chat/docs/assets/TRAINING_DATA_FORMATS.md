# Training data formats
----------------------------------------------------------------------------------------------------


## üìò R√©sum√©

| Format                  | Type mod√®le     | Pros                          | Cons                | LoRA compatible | **LargeLLM (5)**                                                             | **LightLLM (5)**                                                         |
| ----------------------- | --------------- | ----------------------------- | ------------------- | --------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| **SFT (prompt-output)** | Tous            | Simple, efficace              | Peu contextuel      | ‚úÖ               | GPT-J, LLaMA-1, Falcon-40B, Pythia-12B, MPT-30B                              | GPT-2, GPT-Neo-1.3B, TinyLLaMA, Pythia-1B, Phi-1.5                       |
| **ChatML**              | Chat            | Riche, r√¥les distincts        | Format lourd        | ‚úÖ               | GPT-4, GPT-3.5, LLaMA-2-Chat, Mistral-7B-Instruct, DeepSeek-Chat             | Mistral-7B, LLaMA-2-7B-Chat, Zephyr-7B, OpenChat-3.5-7B, DeepSeek-Coder  |
| **Prompt-Completion**   | Base LLM        | Ultra simple                  | Pas conversationnel | ‚úÖ               | Pythia-12B, MPT-30B, LLaMA-1-13B, Falcon-40B, GPT-J                          | GPT-Neo-1.3B, GPT-2, TinyLLaMA, Pythia-1.4B, Phi-2                       |
| **Instruction**         | Instruction     | Donn√©es faciles √† g√©n√©rer     | Biais possible      | ‚úÖ               | WizardLM-13B, Alpaca-13B, LLaMA-2-13B, Vicuna-13B, Baize-13B                 | Alpaca-7B, Vicuna-7B, Zephyr-7B, TinyLLaMA, OpenHermes-2.5-Mistral       |
| **Multi-turn Dialogue** | Chatbot         | Simule des conversations      | Difficile √† g√©n√©rer | ‚úÖ               | Claude-2, ChatGLM3-6B, Yi-34B-Chat, OpenAssistant-12B, Mistral-Instruct-v0.2 | Zephyr-7B, OpenChat-3.5-7B, Mistral-7B-Instruct, MythoMax-L2-13B, Phi-2  |
| **Preference (DPO)**    | Alignement RLHF | Apprentissage des pr√©f√©rences | Co√ªteux √† annoter   | ‚ö†Ô∏è              | GPT-4, Claude-2, Mistral-7B-DPO, LLaMA-2-Chat-70B, Zephyr-Œ≤                  | Zephyr-7B, OpenHermes-2.5-Mistral, DeepSeek-7B-DPO, TinyLLaMA-DPO, Phi-2 |

---

### üìå L√©gende

* **LargeLLM** : Mod√®les puissants, souvent utilis√©s pour inference cloud ou fine-tuning sur GPU haut de gamme.
* **LightLLM** : Parfaits pour fine-tuning local (QLoRA/LoRA), training rapide et √©valuation low-cost.
* **LoRA compatible** : tous les formats ici sont adapt√©s √† des fine-tunings LoRA.

---


## üìö Catalogue des Formats de Donn√©es pour Entra√Ænement LLM & LoRA
----------------------------------------------------------------------------------------------------

Ce guide r√©unit les formats d'entra√Ænement principaux utilis√©s pour les LLMs (grands ou l√©gers), avec :

* Description du format
* Exemple de donn√©es (th√®me : jeux vid√©o)
* Tableau synth√©tique des avantages / inconv√©nients
* Compatibilit√© LoRA
* Mod√®les c√©l√®bres (LargeLLM vs LightLLM)

---

### üîπ 1. **SFT ‚Äî Supervised Fine-Tuning**

**Format :**

```json
{
  "prompt": "...",
  "completion": "..."
}
```

**Utilisation :** apprentissage supervis√© (Dolly, Pythia)

**Exemple :**

```json
{
  "prompt": "Quels sont les meilleurs jeux coop√©ratifs sur PC ?",
  "completion": "It Takes Two, Deep Rock Galactic, Valheim, Left 4 Dead 2."
}
```

| Avantages                          | Inconv√©nients              |
| ---------------------------------- | -------------------------- |
| Simple, clair, facile √† construire | Peu de gestion de contexte |
| Bonne base pour comportement cibl√© | Risque d'overfitting       |
| Compatible avec tous les LLMs      | Structure rigide           |

**LoRA :** ‚úÖ Parfaitement compatible

**LargeLLM :** LLaMA-1, Falcon-40B, GPT-J, MPT-30B, Pythia-12B
**LightLLM :** GPT-2, GPT-Neo, TinyLLaMA, Pythia-1.4B, Phi-1.5

---

### üîπ 2. **ChatML / OpenAI Chat Format**

**Format :**

```json
[
  {"role": "user", "content": "..."},
  {"role": "assistant", "content": "..."}
]
```

**Utilisation :** agents conversationnels (ChatGPT-like)

**Exemple :**

```json
[
  {"role": "user", "content": "Peux-tu me recommander un RPG ?"},
  {"role": "assistant", "content": "Je recommande Divinity: Original Sin 2."}
]
```

| Avantages                                   | Inconv√©nients                              |
| ------------------------------------------- | ------------------------------------------ |
| Format adapt√© aux dialogues naturels        | Lourd √† g√©n√©rer / structurer               |
| Permet de sp√©cifier les r√¥les (system/user) | Requiert un tokenizer avec `chat_template` |

**LoRA :** ‚úÖ Compatible avec tous les LLMs modernes (Mistral, DeepSeek)

**LargeLLM :** GPT-3.5, GPT-4, LLaMA-2-Chat, DeepSeek-Chat, Mistral-Instruct
**LightLLM :** Zephyr, OpenChat, Mistral-7B, LLaMA2-7B, DeepSeek-Coder

---

### üîπ 3. **Prompt-Completion (legacy OpenAI)**

**Format :**

```
Question: ...
Answer: ...
```

**Utilisation :** anciens mod√®les ou base (GPT-2, GPT-Neo)

**Exemple :**

```
Q: C'est quoi un roguelike ?
A: Un jeu avec mort permanente, niveaux g√©n√©r√©s et forte rejouabilit√©.
```

| Avantages                  | Inconv√©nients                      |
| -------------------------- | ---------------------------------- |
| Simple, rapide √† cr√©er     | Pas adapt√© aux interactions riches |
| Peu de formatage √† pr√©voir | Moins pertinent pour chatbots      |

**LoRA :** ‚úÖ Compatible (surtout avec petits mod√®les)

**LargeLLM :** MPT-30B, Pythia-12B, GPT-J, Falcon-40B, LLaMA-1-13B
**LightLLM :** GPT-2, GPT-Neo-1.3B, TinyLLaMA, Phi-2, Pythia-1.4B

---

### üîπ 4. **Instruction Tuning (Alpaca-like)**

**Format :**

```json
{
  "instruction": "...",
  "input": "...",
  "output": "..."
}
```

**Utilisation :** mod√®les orient√©s instruction (Alpaca, WizardLM)

**Exemple :**

```json
{
  "instruction": "Explique ce qu‚Äôest un MMORPG.",
  "input": "",
  "output": "Un MMORPG est un jeu de r√¥le massivement multijoueur..."
}
```

| Avantages                           | Inconv√©nients          |
| ----------------------------------- | ---------------------- |
| Parfait pour les t√¢ches sp√©cifiques | Moins fluide pour chat |
| Facile √† g√©n√©rer avec GPT-3.5       | Peut induire des biais |

**LoRA :** ‚úÖ Id√©al pour QLoRA / PEFT cibl√©

**LargeLLM :** WizardLM, Alpaca-13B, LLaMA-2-13B, Vicuna-13B, Baize
**LightLLM :** Alpaca-7B, Vicuna-7B, Zephyr-7B, OpenHermes, TinyLLaMA

---

### üîπ 5. **Multi-turn Dialogue**

**Format :**

```json
[
  {"speaker": "user", "text": "..."},
  {"speaker": "assistant", "text": "..."},
  ...
]
```

**Utilisation :** simulateur de conversation, chatbot FAQ

**Exemple :**

```json
[
  {"speaker": "user", "text": "Je veux jouer avec ma copine, une id√©e ?"},
  {"speaker": "assistant", "text": "It Takes Two : coop√©ratif, dr√¥le, excellent !"}
]
```

| Avantages                                  | Inconv√©nients                            |
| ------------------------------------------ | ---------------------------------------- |
| R√©aliste, naturel, simule un vrai dialogue | Demande une structure claire et propre   |
| Permet l'apprentissage de contexte         | Contexte limit√© par la taille de fen√™tre |

**LoRA :** ‚úÖ Compatible (attention au context window)

**LargeLLM :** ChatGLM3, Yi-34B-Chat, Claude, OpenAssistant-12B, Mistral-Instruct
**LightLLM :** Zephyr, Mistral-7B, OpenChat-3.5, Phi-2, MythoMax

---

### üîπ 6. **Preference Format (DPO / RLHF)**

**Format :**

```json
{"prompt": "...", "chosen": "...", "rejected": "..."}
```

**Utilisation :** alignement par pr√©f√©rence humaine (DPO)

**Exemple :**

```json
{
  "prompt": "Donne une astuce dans Elden Ring.",
  "chosen": "Utilise les cendres de guerre pour faciliter les boss.",
  "rejected": "Utilise toujours l‚Äôattaque saut√©e, c‚Äôest la meilleure."
}
```

| Avantages                               | Inconv√©nients                       |
| --------------------------------------- | ----------------------------------- |
| Renforce qualit√© et pertinence          | Co√ªteux √† annoter / comparer        |
| Indispensable pour affiner l‚Äôalignement | N√©cessite d√©j√† un mod√®le SFT align√© |

**LoRA :** ‚ö†Ô∏è Possible **apr√®s** un SFT pr√©alable

**LargeLLM :** GPT-4, Claude-2, Mistral-DPO, LLaMA-2-70B, Zephyr-Œ≤
**LightLLM :** Zephyr-7B, DeepSeek-7B-DPO, TinyLLaMA-DPO, Phi-2, OpenHermes-DPO

