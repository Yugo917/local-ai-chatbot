Voici un tableau comparatif complet des principales m√©thodes de **PEFT (Parameter-Efficient Fine-Tuning)** pour les LLMs : **LoRA**, **QLoRA**, **D-LoRA**, **AdaLoRA**, etc., avec leurs **avantages**, **inconv√©nients**, **benchmarks typiques** et **cas d‚Äôusage privil√©gi√©s**.

---

### üîç Comparatif des m√©thodes PEFT pour LLMs

| M√©thode                        | Description courte                                                                              | Avantages                                                                                                        | Inconv√©nients                                                                                                                       | Benchmarks (ex.)                                                                           | Cas d‚Äôusage id√©aux                                                      |
| ------------------------------ | ----------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------- |
| **LoRA** (Low-Rank Adaptation) | Injecte des matrices low-rank dans les poids de l‚Äôattention ou MLP                              | - Tr√®s peu de param√®tres √† fine-tuner<br>- Facilement adaptable<br>- Compatible avec Hugging Face                | - Pas de quantization<br>- Moins efficace en m√©moire que QLoRA                                                                      | - GPT-3.5 avec LoRA atteint des scores proches du full fine-tuning avec <1% des param√®tres | - D√©ploiement en prod<br>- Fine-tuning avec peu de RAM (ex. 24‚Äì32GB)    |
| **QLoRA** (Quantized LoRA)     | Combine quantization (4-bit) et LoRA pour un fine-tuning √† co√ªt ultra faible                    | - RAM minimale (\~24GB pour 65B)<br>- S‚Äôentra√Æne sur un GPU consumer<br>- Tr√®s peu d‚Äôimpact sur les performances | - Temps de training plus long<br>- Incompatible avec certains accelerateurs (ex. XLA)<br>- Moins efficace si full precision requise | - Alpaca-QLoRA (7B) atteint des performances proches de LLaMA-7B full-tune                 | - LLMs > 7B sur machines grand public<br>- Fine-tuning full corpus      |
| **D-LoRA** (Decomposed LoRA)   | Ajoute une s√©paration entre LoRA statique (training) et dynamique (inf√©rence)                   | - R√©duction significative du temps d'inf√©rence<br>- Optimis√© pour serving                                        | - Impl√©mentation plus complexe<br>- Support encore limit√©                                                                           | - D-LoRA 13B \~30% plus rapide en inf√©rence vs LoRA classique                              | - API temps r√©el<br>- Applications embarqu√©es                           |
| **AdaLoRA** (Adaptive LoRA)    | Alloue dynamiquement la capacit√© LoRA aux couches les plus sensibles                            | - Meilleure efficacit√© que LoRA classique<br>- Optimise l‚Äôallocation m√©moire                                     | - Hyperparam√®tres difficiles √† r√©gler<br>- Moins support√© dans les lib standards                                                    | - GPT2 + AdaLoRA > GPT2 + LoRA (en BLEU sur t√¢ches NLG)                                    | - Fine-tuning long ou s√©lectif<br>- Quand la perf. prime sur simplicit√© |
| **LoRA + LLaMA-Adapter**       | Injecte LoRA dans les couches sp√©cifiques d‚Äôun backbone (ex. LLaMA) avec alignement instruction | - Parfait pour instruction tuning<br>- L√©ger & rapide                                                            | - Moins g√©n√©rique<br>- Besoin d‚Äôun dataset bien align√©                                                                              | - Vicuna, Alpaca, etc. utilisent ce m√©canisme                                              | - Agents conversationnels<br>- Chatbots sp√©cialis√©s                     |
| **Full Fine-tuning**           | Re-train de tous les poids                                                                      | - Meilleure performance possible<br>- Compatible avec tous les downstream tasks                                  | - Co√ªt √©norme (RAM, stockage, temps)<br>- Pas scalable                                                                              | - OpenChat, LLaMA2 full fine-tune = top perf                                               | - Labs R\&D<br>- Tr√®s gros budgets                                      |

---

### üìå Remarques importantes

* **QLoRA est aujourd‚Äôhui le meilleur compromis pour le fine-tuning local sur GPU limit√©s** (A100, RTX 3090, 4090‚Ä¶).
* **LoRA** reste plus simple √† d√©ployer en production et compatible avec ONNX / acc√©l√©rateurs.
* **D-LoRA** est int√©ressant quand on veut combiner **fine-tuning efficace + d√©ploiement rapide**.
* **AdaLoRA** brille dans des sc√©narios exigeant **performance fine-grained + contr√¥le avanc√©**.
* Tous ces outils font partie de la famille **PEFT**, port√©e notamment par la lib `peft` de Hugging Face.

---

