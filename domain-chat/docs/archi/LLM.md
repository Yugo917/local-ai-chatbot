### üß† 1. Central API: `LLMApi`

At the core of this module is the `LLMApi`, responsible for coordinating fine-tuning operations on language models (LLMs). It acts as a unified entry point to trigger model customization processes, whether using traditional methods or parameter-efficient approaches (PETF).

---

### üß™ 2. `PETFGenerator`: Parameter-Efficient Fine-Tuning

The `PETFGenerator` module specializes in generating lightweight fine-tuning configurations for LLMs, leveraging "parameter-efficient" techniques such as:

* **LoRA**
* **QLoRA**
* **D-LoRA**
* **AdaLoRA**

These techniques allow for adapting a base LLM to specific use cases using embeddings as training data, while significantly reducing resource requirements (time, memory, cost).

---

### üõ†Ô∏è 3. `FineTunedModelGenerator`: Custom Model Generation

This component uses the fine-tuning configuration generated by `PETFGenerator` to produce a **custom LLM**. It applies the selected method (e.g., QLoRA) to a base model, integrating the provided training dataset, to deliver a model ready for use in specific business contexts (chatbots, search engines, etc.).

---

### üîó 4. Synergy and Use Cases

The full pipeline operates as follows:

1. Domain-specific embeddings are generated beforehand.
2. `PETFGenerator` creates a fine-tuned adapter for a target LLM based on those embeddings.
3. `FineTunedModelGenerator` applies this adapter to the base model to produce a deployment-ready LLM.

This design enables scalable, efficient, and modular LLM customization, adaptable to a wide range of contexts (industries, languages, tones, etc.).